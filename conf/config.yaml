output_dir: "/content/drive/MyDrive/MIR/train"
load_dir: "model_with_best_eval" #"epoch_most_recent" #

dataset:
  name: null
  tokens:
    s_max: 15
    n_max: 60
    encoder_n_max: 60

  train:
    image_dir: "/content/sample_data/data/images/images_normalized"
    caption_json: "/content/drive/MyDrive/MIR/train.json" #"/kaggle/input/custom/train.json" #
    #history_json: null
    #file_list: null

  eval:
    image_dir: "/content/sample_data/data/images/images_normalized"
    caption_json: "/content/drive/MyDrive/MIR/val.json" #"/kaggle/input/custom/val.json" #
    #history_json: null
    #file_list: null

model:
  name: null
  config_name: null
  build_model: true
  from_trained: false
  from_checkpoint: false

tokenizer:
  name: '/content/sample_data/MIR/tokenizers/bpe_tokenizer.json'
  use_fast: false
  preprocessing_num_workers: null

# vocabs:
#   name1: null
#   name2: null
  
training:
  seed: 15
  shuffle: false
  learning_rate: 5e-4
  weight_decay: 0.1
  num_epochs: 30
  max_train_steps: 200000
  gradient_accumulation_steps: 32
  lr_scheduler: "cosine" # ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
  lr_warmup_steps: 500
  eval_every: 64
  save_every: 768
  img_size: (224,224)
  train_batch_size: 2
  eval_batch_size: 2

  checkpoint:
    resume_from_checkpoint: 0 # integer representing which checkpoint to load from, or <= 0 to not
    every_n_steps: null


architecture:
  #encoder:
  model_name: "convnext_l" #"densenet201"
  pretrained: true
  hidden_layer_size: 256 # Number of units in the hidden layer of the MLP
  classes: 210
  add_encoder: true
  # state_path:
  #   cnn_model: null
  #   mlc: null

  #history_encoder:
  history_encoder_num_layers: 12
  history_encoder_n_heads: 16
  history_encoder_dim_feedforward: 2048
  #history_encoder_dmodel: 256 
  use_history: true

  #prompt_attention:
  pa_nhead: 1
  use_residual: true


  #semantic_extractor:
  semantic_features_dim: 512
  k: 8
  #state_path: null

  #attention:
  co_attention: false
  features_dim: 49 # visual features dim
  #hidden_dim: 512 # must be the same for hidden size of lstm in sentence lstm
  att_units: 768 #512 # Number of linear units for the attention network
  #semantic_dim: 512 # Must be the same as semantic_features_dim in semantic_extractor
  

  #sent_lstm:
  #hidden_layer_size: 128 # Number of units for the hidden layer of the MLP
  hidden_dim: 1024 #768
  lstm_layers: 10 #Num of lstm layers
  #features_dim: 2048 # visual features dim (not number of channels)
  enforce_info: false
  

  #decoder:
  dec_num_layers: 16
  vocab_size: 6287
  use_topic_per_layer: [false, false, false, false, false, false, false, true, true, true,true, true, true,true ,false, false] # 
  use_cross_att_per_layer: [false, false, false, false, false, false, false, true, true, true, true,true, true, false, false, false] # 
  use_prompt_per_layer: [false,false, false, false, false, false, false, false, true, false,true,false, true, false, false, false] #
  d_model: 1024 #768
  nhead: 16 #12
  #mem_dim: 2048 # visual features dim (not number of channels)
  dim_feedforward: 2048
  #topic_emb:  512 # must be the same as hidden_size for lstm 
  topic_units:  1024 #768 #256 # Number of units for the topic's linear transform network in each layer
  dropout: 0.1
  activation: gelu
  layer_norm_eps: 1e-5
  batch_first: true
  norm_first: true
  device: "cuda" #'cuda' # previously None
  dtype: null
  norm: null
  #state_path: null

tracking: false